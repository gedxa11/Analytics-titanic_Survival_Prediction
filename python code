import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score
from sklearn.tree import plot_tree
data = pd.read_csv("C:/Users/geral/Downloads/archive/Titanic-Dataset.csv")
print(data.head())
print(data.shape)
# %%

print('Missing values:\n', data.isnull().sum())
plt.figure(figsize=(6,4)); data.isnull().sum().plot(kind='bar'); plt.title('Missing values per column'); plt.tight_layout(); plt.show()
plt.figure(figsize=(5,4)); data['Survived'].value_counts().plot(kind='bar'); plt.title('Survived distribution'); plt.tight_layout(); plt.show()
df = data.copy()
df = df.drop(columns=['PassengerId','Name','Ticket','Cabin'])
print(df.head())
y = df['Survived']
X = df.drop(columns=['Survived'])
# %%

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()
num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')
if num_cols:
    X[num_cols] = num_imputer.fit_transform(X[num_cols])
if cat_cols:
    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])
for col in cat_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
scaler = StandardScaler()
if num_cols:
    X[num_cols] = scaler.fit_transform(X[num_cols])
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)
# %%

sns.set_theme(style="whitegrid", palette="pastel")

fig, axs = plt.subplots(1, 3, figsize=(18, 5))
sns.histplot(df['Age'], bins=25, kde=True, ax=axs[0], color='skyblue'); axs[0].set_title('Age Distribution')
sns.histplot(df['Fare'], bins=25, kde=True, ax=axs[1], color='lightgreen'); axs[1].set_title('Fare Distribution')
sns.countplot(x='Survived', data=df, ax=axs[2], palette='pastel'); axs[2].set_title('Survival Count')
plt.suptitle('Distribution of Age, Fare, and Survival', fontsize=14); plt.tight_layout(); plt.show()

fig, axs = plt.subplots(1, 2, figsize=(12, 5))
sns.barplot(x='Sex', y='Survived', data=df, ax=axs[0], palette='coolwarm')
sns.barplot(x='Pclass', y='Survived', data=df, ax=axs[1], palette='viridis')
axs[0].set_title('Survival Rate by Gender'); axs[1].set_title('Survival Rate by Passenger Class')
plt.tight_layout(); plt.show()

fig, axs = plt.subplots(1, 2, figsize=(14, 5))
sns.kdeplot(data=df, x='Age', hue='Survived', fill=True, ax=axs[0], palette='Set2')
sns.scatterplot(x='Fare', y='Age', hue='Survived', data=df, ax=axs[1], palette='coolwarm', alpha=0.7)
axs[0].set_title('Age Distribution by Survival'); axs[1].set_title('Fare vs Age by Survival')
plt.tight_layout(); plt.show()

plt.figure(figsize=(9, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Heatmap'); plt.show()

fig, ax = plt.subplots(figsize=(10, 4)); ax.axis('off')
steps = ["Data Cleaning", "Feature Engineering", "Train-Test Split", "Model Training", "Evaluation"]
x_positions = [0.1, 0.3, 0.5, 0.7, 0.9]
for step, x in zip(steps, x_positions):
    ax.add_patch(mpatches.FancyBboxPatch((x-0.07, 0.45), 0.14, 0.1, boxstyle="round,pad=0.02", fc="skyblue", ec="navy"))
    ax.text(x, 0.5, step, ha='center', va='center', fontsize=10)
for i in range(len(x_positions)-1):
    ax.annotate('', xy=(x_positions[i+1]-0.09, 0.5), xytext=(x_positions[i]+0.09, 0.5),
                arrowprops=dict(arrowstyle="->", lw=2, color='gray'))
plt.title("Model Training Pipeline", fontsize=13, weight='bold'); plt.show()
# %%

models = {}
# GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
models['GaussianNB'] = gnb
# KNN
knn_params = {'n_neighbors': list(range(3,12,2)), 'weights': ['uniform','distance']}
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
knn_grid.fit(X_train, y_train)
models['KNN'] = knn_grid.best_estimator_
# Decision Tree
dt_params = {'max_depth':[3,4,5,6,8,None], 'min_samples_split':[2,5,10]}
dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5)
dt_grid.fit(X_train, y_train)
models['DecisionTree'] = dt_grid.best_estimator_
# %%

for name, model in models.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') else None
    print(name)
    print('Accuracy:', accuracy_score(y_test, y_pred))
    print('Precision:', precision_score(y_test, y_pred, zero_division=0))
    print('Recall:', recall_score(y_test, y_pred, zero_division=0))
    print('F1:', f1_score(y_test, y_pred, zero_division=0))
    if y_prob is not None:
        print('AUC:', roc_auc_score(y_test, y_prob))
    print('\n')
# %%
results = []

print("Evaluating trained models...\n")

for name, model in models.items():
    y_pred = model.predict(X_test)

    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_proba)
    else:
        auc = None

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "F1-Score": round(f1, 4),
        "AUC": round(auc, 4) if auc is not None else "N/A"
    })

df_results = pd.DataFrame(results)

df_results = df_results.sort_values(by="Accuracy", ascending=False)

print("Model Evaluation Summary:")
print(df_results)

df_results.to_csv("model_results_summary.csv", index=False)

print("\nâœ… Results exported successfully to 'model_results_summary.csv'.")
# %%

results_df = pd.read_csv("model_results_summary.csv", index_col=0)
results_df
# %%

if 'accuracy' in results_df.columns:
    acc_col = 'accuracy'
elif 'Accuracy' in results_df.columns:
    acc_col = 'Accuracy'
else:
    acc_col = results_df.columns[0]  # fallback

print("Model Accuracies:")
print(results_df[acc_col])

plt.figure(figsize=(7,5))
plt.bar(results_df.index, results_df[acc_col], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
for i, v in enumerate(results_df[acc_col]):
    plt.text(i, v + 0.01, f"{v:.3f}", ha='center', fontsize=10)
plt.tight_layout()
plt.show()

# %%

def plot_confusion(cm, title="Confusion Matrix"):
    fig, ax = plt.subplots(figsize=(4,3))
    im = ax.imshow(cm, interpolation='nearest')
    ax.set_title(title)
    ax.set_xlabel('Predicted label'); ax.set_ylabel('True label')
    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(['0','1']); ax.set_yticklabels(['0','1'])
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i, j], ha='center', va='center',
                    color='white' if cm[i,j] > cm.max()/2 else 'black')
    plt.colorbar(im)
    plt.tight_layout()
    return fig
# %%

for name, model in models.items():
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    fig = plot_confusion(cm, title=f"{name} Confusion Matrix")
    plt.show()
# %%

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:,1]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        auc = roc_auc_score(y_test, y_proba)
        plt.figure(figsize=(5,4))
        plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')
        plt.plot([0,1],[0,1],'--', color='gray')
        plt.title(f"{name} ROC Curve (AUC = {auc:.3f})")
        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
        plt.legend(loc='lower right')
        plt.tight_layout()
        plt.show()
    else:
        print(f"{name} does not support predict_proba()")
# %%

plt.figure(figsize=(7,6))
for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:,1]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        auc = roc_auc_score(y_test, y_proba)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")
plt.plot([0,1],[0,1],'--', color='gray')
plt.title("ROC Curve Comparison")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()
# %%

acc_values = {name: accuracy_score(y_test, model.predict(X_test)) for name, model in models.items()}
plt.figure(figsize=(7,5))
bars = plt.bar(acc_values.keys(), acc_values.values(), color=['skyblue','lightgreen','salmon'])
plt.ylim(0,1); plt.ylabel('Accuracy'); plt.title('Model Accuracy Comparison')
for bar in bars:
    h = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, h+0.01, f"{h:.3f}", ha='center')
plt.tight_layout()
plt.show()
# %%

tree_model = models.get('DecisionTree', None)
if tree_model is not None:
    plt.figure(figsize=(12,8))
    plot_tree(tree_model, filled=True, feature_names=X_test.columns, max_depth=3)
    plt.title("Decision Tree Visualization (First 3 Levels)")
    plt.show()

    # Feature Importances
    importances = tree_model.feature_importances_
    fi = pd.Series(importances, index=X_test.columns).sort_values(ascending=False)
    plt.figure(figsize=(6,4))
    plt.bar(fi.index, fi.values)
    plt.xticks(rotation=45, ha='right')
    plt.title("Decision Tree Feature Importances")
    plt.tight_layout()
    plt.show()
